\documentclass{beamer}
\usepackage{graphicx,psfrag,color,upquote}
\usepackage{lmodern}

\input talk_defs.tex
\input formatting.tex

\mode<presentation>
{
\usetheme{default}
}

\newcommand{\dist}{\mathop{\bf dist{}}}

% \raggedright
% \special{! TeXDict begin /landplus90{true}store end }
% \definecolor{bluegray}{rgb}{0.15,0.20,0.40}
% \definecolor{bluegraylight}{rgb}{0.35,0.40,0.60}
% \definecolor{medgray}{rgb}{0.4,0.4,0.4}
% \definecolor{gray}{rgb}{0.35,0.35,0.35}
% \definecolor{lightgray}{rgb}{0.7,0.7,0.7}
\definecolor{darkblue}{rgb}{0.2,0.2,1.0}
\definecolor{darkgreen}{rgb}{0.0,0.5,0.3}
%\definecolor{greengray}{rgb}{0.05,0.20,0.05}
%\newcommand{\BGE}[1]{\textbf{\textcolor{bluegray}{#1}}} %bluegray emph
%\renewcommand{\labelitemi}{\textcolor{red}\textbullet}
%\renewcommand{\labelitemii}{\textcolor{red}{--}}
%\renewcommand{\end{frame} \begin{frame}}[1]{\foilhead[-1.0cm]{#1}}
%\newcommand{\end{frame} \begin{frame}}[1]{\foilhead[-1.0cm]{\textcolor{red}{#1}}}

\definecolor{pdefvalue}{rgb}{0.467,0.000,0.533}
\newcommand{\pdefvalue}[1]{\textcolor{pdefvalue}{#1}}
\definecolor{pdefidentifier}{rgb}{0.000,0.000,0.000}
\newcommand{\pdefidentifier}[1]{\textcolor{pdefidentifier}{#1}}
\definecolor{pdefcomment}{rgb}{0.502,0.502,0.502}
\newcommand{\pdefcomment}[1]{\textcolor{pdefcomment}{#1}}
\definecolor{pdefblock}{rgb}{0.267,0.533,0.867}
\newcommand{\pdefblock}[1]{\textcolor{pdefblock}{#1}}
\definecolor{pdeferror}{rgb}{0.667,0.000,0.000}
\newcommand{\pdeferror}[1]{\textcolor{pdeferror}{#1}}
\definecolor{pdefequals}{rgb}{0.000,0.490,0.000}
\newcommand{\pdefequals}[1]{\textcolor{pdefequals}{#1}}
\definecolor{pdefdim}{rgb}{0.973,0.502,0.090}
\newcommand{\pdefdim}[1]{\textcolor{pdefdim}{#1}}
\definecolor{pdefattr}{rgb}{0.000,0.000,0.467}
\newcommand{\pdefattr}[1]{\textcolor{pdefattr}{#1}}
\definecolor{pdefconstr}{rgb}{0.000,0.000,0.000}
\newcommand{\pdefconstr}[1]{\textcolor{pdefconstr}{#1}}
\definecolor{pdefobjv}{rgb}{0.000,0.000,0.000}
\newcommand{\pdefobjv}[1]{\textcolor{pdefobjv}{#1}}
\definecolor{pdefconstrsign}{rgb}{0.000,0.000,0.800}
\newcommand{\pdefconstrsign}[1]{\textcolor{pdefconstrsign}{#1}}
\definecolor{pdefrange}{rgb}{0.333,0.467,0.200}
\newcommand{\pdefrange}[1]{\textcolor{pdefrange}{#1}}
\definecolor{pdefindex}{rgb}{0.333,0.467,0.200}
\newcommand{\pdefindex}[1]{\textcolor{pdefindex}{#1}}
\definecolor{pdefbrackets}{rgb}{0.333,0.467,0.200}
\newcommand{\pdefbrackets}[1]{\textcolor{pdefbrackets}{#1}}
\definecolor{pdefsemicolon}{rgb}{0.133,0.400,0.800}
\newcommand{\pdefsemicolon}[1]{\textcolor{pdefsemicolon}{#1}}
\definecolor{pdeffunction}{rgb}{0.000,0.333,0.667}
\newcommand{\pdeffunction}[1]{\textcolor{pdeffunction}{#1}}
\definecolor{pdefdenom}{rgb}{0.000,0.333,0.667}
\newcommand{\pdefdenom}[1]{\textcolor{pdefdenom}{#1}}
\definecolor{pdefsparseindices}{rgb}{0.667,0.000,0.533}
\newcommand{\pdefsparseindices}[1]{\textcolor{pdefsparseindices}{#1}}

\title{Disciplined Convex Optimization with CVXR}

\author{\textbf{Anqi Fu} \and Bala Narasimhan \and Stephen Boyd \\[2ex]
	EE \& Statistics Departments\\[1ex]
	Stanford University}
\date{useR! Conference 2016}

\begin{document}
	
\begin{frame}
	\titlepage
\end{frame}

\begin{frame}
	\tableofcontents
\end{frame}

\section{Convex Optimization}

\begin{frame}{Convex Optimization}% problem --- standard form}
	
	\[
	\begin{array}{ll} \mbox{minimize} & f_0(x)\\
	\mbox{subject to} & f_i(x) \leq 0, \quad i=1, \ldots, m\\
	& Ax=b
	\end{array}
	\]
	with variable $x \in \reals^n$
	
	\BIT
		\item Objective and inequality constraints $f_0, \ldots, f_m$ are convex %\\[1ex]
		%for all $x$, $y$, $\theta \in [0,1]$,
		%\[
		%f_i(\theta x + (1-\theta) y) \leq \theta f_i(x) + (1-\theta) f_i(y)
		%\]
		%\ie, graphs of $f_i$ curve upward
		\item Equality constraints are linear
	\EIT
	\pause
	
	\vfill
	Why?
	\BIT
		\item We can solve convex optimization problems
		\item There are many applications in many fields, including machine learning and statistics
	\EIT
	
\end{frame}

\begin{frame}{Convex Problems in Statistics}
	%Existing packages for these, but new methods every years (see SigKDD)
	\BIT
		\item (Non-negative) Least squares
		\item Ridge and lasso regression
		\item Logistic regression
		\item Isotonic regression
		\item Huber (robust) regression
		\item Maximum entropy and related problems
		\item Support vector machines
		\item Sparse inverse covariance
		\item \ldots and new methods being invented every year!
	\EIT
\end{frame}

\begin{frame}[fragile]{Domain Specific Languages}
	% What's changed in last ~10 years is DSLs. Not necessary to descend to gradient or Hessian level. Tens of thousands of people use these packages.
	\BIT
		\item CVX, CVXPY, YALMIP, Convex.jl
		\item Slower than custom code, but extremely flexible and enables fast prototyping
	\EIT
	
	\pause
	\begin{verbatim}
	from cvxpy import *
	beta = Variable(n)
	cost = norm(y - X * beta)
	prob = Problem(Minimize(cost))
	prob.solve()
	beta.value
	\end{verbatim}
\end{frame}

\iffalse
\begin{frame}{Examples}
	\BIT
	\item Least-squares, least-squares with $\ell_1$ regularization (lasso)
	\item Linear program (LP), quadratic program (QP)
	\item Second-order cone program (SOCP)
	\item Semidefinite program (SDP)
	\item Maximum entropy and related problems
	\item Support vector machine
	\EIT
\end{frame}
\fi

\iffalse
\begin{frame}{Convex Optimization Problem --- Conic Form}
	Cone program:
	\[
	\begin{array}{ll} \mbox{minimize} & c^Tx\\
	\mbox{subject to} & Ax = b, \quad x \in \mathcal K
	\end{array}
	\]
	with variable $x \in \reals^n$
	
	\BIT
	\item Linear objective, equality constraints;
	$\mathcal K$ is convex cone
	%\BIT
	%\item $x \in \mathcal K$ is a generalized nonnegativity constraint
	%\EIT
	\item Special cases:
	\BIT
	\item Linear program (LP)
	%second-order cone program (SOCP): $\mathcal K = $
	%$\mathcal K=\symm^n_{+}$: %(PSD matrices)
	\item Semidefinite program (SDP)
	\EIT
	\vfill
	\item The modern canonical form
	\item \emph{There are well developed solvers for cone programs}
	\EIT
\end{frame}
\fi

\iffalse
\begin{frame}{Why Convex Optimization?}
	\BIT
	\item Beautiful, fairly complete, and useful theory
	%\pause
	\item Solution algorithms that work well in theory and practice
	\BIT
	\item Convex optimization is \textbf{actionable}
	\EIT
	%\pause
	\item \textbf{Many applications} in
	\BIT
	\item Control
	\item Combinatorial optimization
	\item Signal and image processing
	\item Communications, networks
	\item Circuit design
	\item Machine learning, statistics
	\item Finance
	\EIT
	%\ldots and many more
	\EIT
\end{frame}
\fi

\iffalse
\begin{frame}{How do you Solve a Convex Problem?}
	\BIT\itemsep 20pt
	\item Use an existing custom solver for your specific problem
	%(\eg, SVM, lasso)
	%\item write your own (custom) solver
	%\BIT
	%\item lots of work, but can take advantage of special structure
	%\item commonly done in machine learning
	%\EIT
	\item Develop a new solver for your problem using a currently
	fashionable method
	%(mirror descent, Nesterov acceleration, Frank-Wolf \ldots)
	\BIT
	\item Requires work
	\item But (with luck) will scale to large problems
	\EIT
	\item Transform your problem into a cone program, and use a standard cone program solver
	\BIT
	\item Can be \emph{automated} using \emph{domain specific languages}
	\item CVX, YALMIP, CVXPY, Convex.jl
	\EIT
	
	%\BIT
	%\item extends reach of problems solvable by standard solvers
	%\item transformation can be hard to find, cumbersome to carry out
	%\EIT
	%\item \textbf{this talk:} methods to formalize and automate last
	%approach
	\EIT
\end{frame}
\fi

\section{CVXR}

\begin{frame}{CVXR}
	% Math isn't new. What's new is ability to implement in a few lines of a clear code.
	A modeling language in R for convex optimization
	
	\BIT
		\item Connects to many open source solvers
		%\item Uses disciplined convex programming to verify convexity
		\item Supports parameters, multiple constraints
		\item Mixes easily with general R code and other libraries
	\EIT
\end{frame}

\section{Examples}

\begin{frame}[fragile]{Ordinary Least Squares (OLS)}
	\BIT
		\item minimize $||X\beta - y||_2^2$
		\item $\beta \in \reals^m$ is variable, $X$ and $y$ are constants
	\EIT
	
	\pause
	\begin{verbatim}
	library(cvxr)
	beta <- Variable(m)
	obj <- SumSquares(y - X %*% beta)
	prob <- Problem(Minimize(obj))
	solution <- solve(prob)
	solution$opt_val
	solution$beta
	\end{verbatim}
	
	\BIT
		\item \verb|X| and \verb|y| are constants; \verb|beta|, \verb|obj|, and \verb|prob| are S4 objects
		\item \verb|solve| method returns a list that includes optimal \verb|beta|
	\EIT
\end{frame}

\begin{frame}[fragile]{Non-Negative Least Squares (NNLS)}
	\BIT
	\item minimize $||X\beta - y||_2^2$ subject to $\beta \geq 0$
	\EIT
	\pause
	\begin{verbatim}
	constr <- list(beta >= 0)
	prob2 <- Problem(Minimize(obj), constr)
	solution2 <- solve(prob2)
	solution2$opt_val
	solution2$beta
	\end{verbatim}
	
	\BIT
		\item Construct new problem with list \verb|constr| of constraints formed from constants and variables
		\item Variables, parameters, expressions, and constraints exist outside of any problem
	\EIT
\end{frame}

\begin{frame}{True Coefficients vs. OLS}
	\vfill
	\begin{figure}
		\includegraphics[width=1\textwidth]{figs/nnls1.png}
	\end{figure}
	\vfill
\end{frame}

\begin{frame}{True Coefficients vs. NNLS}
	\vfill
	\begin{figure}
		\includegraphics[width=1\textwidth]{figs/nnls2.png}
	\end{figure}
	\vfill
\end{frame}

\iffalse
\begin{frame}[fragile]{Huber Regression}
	\[
	\begin{array}{ll}
	\mbox{minimize} & \sum_i^n \phi(y_i - \beta^T x_i)
	\end{array}
	\]
	with variable $\beta \in \reals^n$ and Huber function
	\[
	\begin{array}{ll}
	\phi(u) &= \begin{cases} u^2 & |u| \leq 0 \\
							2Mu - M^2 & |u| > 0 \end{cases}
	\end{array}
	\]
	where $M > 0$ is the threshold
	\BIT
		\item TODO: Plot of Huber function?
		\item Same as OLS for small residuals, allows some large residuals
		\item Better fit when data contains outliers
	\EIT
\end{frame}
\fi

\iffalse
\begin{frame}[fragile]{Huber Regression}
	\begin{verbatim}
	beta = Variable(m)
	cost = SumEntries(Huber(y - X %*% beta, 1))
	prob <- Problem(Minimize(cost))
	solution <- solve(prob)
	solution$opt_val
	solution$beta
	\end{verbatim}
	
	\BIT
		\item Generate data and replace fraction $p$ of $y_i$'s with $-y_i$
		\item Huber regression with threshold of 1, no constraints
		\item Compare with OLS and prescient regression where sign changes are known
	\EIT
\end{frame}
\fi

\iffalse
\begin{frame}{Relative Reconstruction Error}
	\vfill
	\begin{figure}
		\includegraphics[width=1\textwidth]{figs/huber1.png}
	\end{figure}
	\vfill
\end{frame}
\fi

\iffalse
\begin{frame}{Relative Reconstruction Error}
	\vfill
	\begin{figure}
		\includegraphics[width=1\textwidth]{figs/huber2.png}
	\end{figure}
	\vfill
\end{frame}
\fi

\begin{frame}{Direct Standardization}
	%Given non-uniform samples $X,y$ and known expectations for features in $X$, guess distribution of $y$
	\BIT
		\item Samples $(X,y)$ drawn \textbf{non-uniformly} from a distribution
		\item Expectations of features of $X$ are a known quantity $b \in \reals^m$
		\item Want to estimate probability $p \in \reals^n$ for all samples
		\item Choose $p_i$ to match known expectations, while maximizing total entropy
	\EIT
	
	\pause
	\[
	\begin{array}{ll} \mbox{maximize} & \sum_i^n \mbox{entr}(p_i) \\
	\mbox{subject to} & p \geq 0 \quad \ones^Tp = 1 \quad X^Tp = b
	\end{array}
	\]
	
	\BIT
	\item $\mbox{entr}(p) = -p_i \log p_i$
	\item $(y,p)$ is an estimate of true distribution of the response
	\EIT
\end{frame}

\begin{frame}[fragile]{Direct Standardization}
	\begin{verbatim}
	probs <- Variable(n)
	cost <- SumEntries(Entr(probs))
	constr <- list(probs >= 0, SumEntries(probs) == 1, 
				   t(X) %*% probs == b)
	prob <- Problem(Maximize(cost), constr)
	solution <- solve(prob)
	solution$probs
	\end{verbatim}
	
	\BIT
		\item \verb|Entr| is the elementwise entropy function
		\item \verb|solution$probs| is an R vector of sample probabilities
	\EIT
\end{frame}

\begin{frame}{True vs. Sample Cumulative Distribution}
	\vfill
	\begin{figure}
		\includegraphics[width=1\textwidth]{figs/dstand1.png}
	\end{figure}
	\vfill
\end{frame}

\begin{frame}{True vs. Estimated Cumulative Distribution}
	\vfill
	\begin{figure}
		\includegraphics[width=1\textwidth]{figs/dstand2.png}
	\end{figure}
	\vfill
\end{frame}

%\section{Disciplined Convex Programming}

\iffalse
\begin{frame}{Curvature: Convex, Concave, and Affine functions}
	\includegraphics[width=0.9\textwidth,height=0.20\textheight]{figs/curvature.png}
	\BIT
		\item $f$ is \emph{concave} if $-f$ is convex, \ie, for any $x,y$,
		$\theta \in [0,1]$,
		\[
		f(\theta x + (1-\theta) y) \geq \theta f(x) + (1-\theta) f(y)
		\]
		\item $f$ is \emph{affine} if it is convex and concave, \ie,
		\[
		f(\theta x + (1-\theta) y) = \theta f(x) + (1-\theta) f(y)
		\]
		for any $x,y$, $\theta \in [0,1]$
		\item $f$ is affine $\Longleftrightarrow$ it has form
		$f(x) = a^Tx +b$
	\EIT
\end{frame}
\fi

\iffalse
\begin{frame}{Verifying a Function is Convex or Concave}
	(Verifying affine is easy)
	
	\vfill
	Approaches:
	\BIT
		\item Via basic definition (inequality)
		\item Via first or second order conditions, \eg, $\nabla^2 f(x) \succeq 0$
		
		\vfill
		
		\item Via convex calculus: construct $f$ using
		\BIT
			\item Library of basic functions that are convex or concave
			\item Calculus rules or transformations that preserve convexity
		\EIT
	\EIT
\end{frame}
\fi

\iffalse
\begin{frame}{Disciplined Convex Programming (DCP)}
	(\emph{Grant, Boyd, Ye, 2006})
	
	\vfill
	\BIT
		\item Framework for describing convex optimization problems
		\item Based on constructive convex analysis
		\item Sufficient, but not necessary for convexity
		\item Basis for several domain-specific languages and tools for convex optimization
		\BIT
			\item CVX, YALMIP, CVXPY, Convex.jl
		\EIT
	\EIT
	\vfill
\end{frame}
\fi

\section{Future Work}
\begin{frame}{Future Work}
	\BIT
		\item More solvers - SCS, CVXOPT, etc
		\item More convex functions and constraints
		\item Warm start to speed up convergence
	\EIT
\end{frame}

\end{document}