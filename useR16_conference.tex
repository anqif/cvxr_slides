\documentclass{beamer}
\usepackage{graphicx,psfrag,color,upquote}
\usepackage{lmodern}

\input talk_defs.tex
\input formatting.tex

\mode<presentation>
{
\usetheme{default}
}

\newcommand{\dist}{\mathop{\bf dist{}}}

% \raggedright
% \special{! TeXDict begin /landplus90{true}store end }
% \definecolor{bluegray}{rgb}{0.15,0.20,0.40}
% \definecolor{bluegraylight}{rgb}{0.35,0.40,0.60}
% \definecolor{medgray}{rgb}{0.4,0.4,0.4}
% \definecolor{gray}{rgb}{0.35,0.35,0.35}
% \definecolor{lightgray}{rgb}{0.7,0.7,0.7}
\definecolor{darkblue}{rgb}{0.2,0.2,1.0}
\definecolor{darkgreen}{rgb}{0.0,0.5,0.3}
%\definecolor{greengray}{rgb}{0.05,0.20,0.05}
%\newcommand{\BGE}[1]{\textbf{\textcolor{bluegray}{#1}}} %bluegray emph
%\renewcommand{\labelitemi}{\textcolor{red}\textbullet}
%\renewcommand{\labelitemii}{\textcolor{red}{--}}
%\renewcommand{\end{frame} \begin{frame}}[1]{\foilhead[-1.0cm]{#1}}
%\newcommand{\end{frame} \begin{frame}}[1]{\foilhead[-1.0cm]{\textcolor{red}{#1}}}

\definecolor{pdefvalue}{rgb}{0.467,0.000,0.533}
\newcommand{\pdefvalue}[1]{\textcolor{pdefvalue}{#1}}
\definecolor{pdefidentifier}{rgb}{0.000,0.000,0.000}
\newcommand{\pdefidentifier}[1]{\textcolor{pdefidentifier}{#1}}
\definecolor{pdefcomment}{rgb}{0.502,0.502,0.502}
\newcommand{\pdefcomment}[1]{\textcolor{pdefcomment}{#1}}
\definecolor{pdefblock}{rgb}{0.267,0.533,0.867}
\newcommand{\pdefblock}[1]{\textcolor{pdefblock}{#1}}
\definecolor{pdeferror}{rgb}{0.667,0.000,0.000}
\newcommand{\pdeferror}[1]{\textcolor{pdeferror}{#1}}
\definecolor{pdefequals}{rgb}{0.000,0.490,0.000}
\newcommand{\pdefequals}[1]{\textcolor{pdefequals}{#1}}
\definecolor{pdefdim}{rgb}{0.973,0.502,0.090}
\newcommand{\pdefdim}[1]{\textcolor{pdefdim}{#1}}
\definecolor{pdefattr}{rgb}{0.000,0.000,0.467}
\newcommand{\pdefattr}[1]{\textcolor{pdefattr}{#1}}
\definecolor{pdefconstr}{rgb}{0.000,0.000,0.000}
\newcommand{\pdefconstr}[1]{\textcolor{pdefconstr}{#1}}
\definecolor{pdefobjv}{rgb}{0.000,0.000,0.000}
\newcommand{\pdefobjv}[1]{\textcolor{pdefobjv}{#1}}
\definecolor{pdefconstrsign}{rgb}{0.000,0.000,0.800}
\newcommand{\pdefconstrsign}[1]{\textcolor{pdefconstrsign}{#1}}
\definecolor{pdefrange}{rgb}{0.333,0.467,0.200}
\newcommand{\pdefrange}[1]{\textcolor{pdefrange}{#1}}
\definecolor{pdefindex}{rgb}{0.333,0.467,0.200}
\newcommand{\pdefindex}[1]{\textcolor{pdefindex}{#1}}
\definecolor{pdefbrackets}{rgb}{0.333,0.467,0.200}
\newcommand{\pdefbrackets}[1]{\textcolor{pdefbrackets}{#1}}
\definecolor{pdefsemicolon}{rgb}{0.133,0.400,0.800}
\newcommand{\pdefsemicolon}[1]{\textcolor{pdefsemicolon}{#1}}
\definecolor{pdeffunction}{rgb}{0.000,0.333,0.667}
\newcommand{\pdeffunction}[1]{\textcolor{pdeffunction}{#1}}
\definecolor{pdefdenom}{rgb}{0.000,0.333,0.667}
\newcommand{\pdefdenom}[1]{\textcolor{pdefdenom}{#1}}
\definecolor{pdefsparseindices}{rgb}{0.667,0.000,0.533}
\newcommand{\pdefsparseindices}[1]{\textcolor{pdefsparseindices}{#1}}

\title{Disciplined Convex Optimization with CVXR}

\author{\textbf{Anqi Fu} \and Bala Narasimhan \and Stephen Boyd \\[2ex]
	EE \& Statistics Departments\\[1ex]
	Stanford University}
\date{useR! Conference 2016}

\begin{document}
	
\begin{frame}
	\titlepage
\end{frame}

\begin{frame}
	\tableofcontents
\end{frame}

\section{Convex Optimization}

\begin{frame}{Convex Optimization}% problem --- standard form}
	
	\[
	\begin{array}{ll} \mbox{minimize} & f_0(x)\\
	\mbox{subject to} & f_i(x) \leq 0, \quad i=1, \ldots, m\\
	& Ax=b
	\end{array}
	\]
	with variable $x \in \reals^n$
	
	\BIT
		\item Objective and inequality constraints $f_0, \ldots, f_m$ are convex %\\[1ex]
		%for all $x$, $y$, $\theta \in [0,1]$,
		%\[
		%f_i(\theta x + (1-\theta) y) \leq \theta f_i(x) + (1-\theta) f_i(y)
		%\]
		%\ie, graphs of $f_i$ curve upward
		\item Equality constraints are linear
	\EIT
	\pause
	
	\vfill
	Why?
	\BIT
		\item We can solve convex optimization problems
		\item There are many applications in many fields
	\EIT
	
\end{frame}

\iffalse
\begin{frame}{Examples}
	\BIT
	\item Least-squares, least-squares with $\ell_1$ regularization (lasso)
	\item Linear program (LP), quadratic program (QP)
	\item Second-order cone program (SOCP)
	\item Semidefinite program (SDP)
	\item Maximum entropy and related problems
	\item Support vector machine
	\EIT
\end{frame}
\fi

\iffalse
\begin{frame}{Convex Optimization Problem --- Conic Form}
	Cone program:
	\[
	\begin{array}{ll} \mbox{minimize} & c^Tx\\
	\mbox{subject to} & Ax = b, \quad x \in \mathcal K
	\end{array}
	\]
	with variable $x \in \reals^n$
	
	\BIT
	\item Linear objective, equality constraints;
	$\mathcal K$ is convex cone
	%\BIT
	%\item $x \in \mathcal K$ is a generalized nonnegativity constraint
	%\EIT
	\item Special cases:
	\BIT
	\item Linear program (LP)
	%second-order cone program (SOCP): $\mathcal K = $
	%$\mathcal K=\symm^n_{+}$: %(PSD matrices)
	\item Semidefinite program (SDP)
	\EIT
	\vfill
	\item The modern canonical form
	\item \emph{There are well developed solvers for cone programs}
	\EIT
\end{frame}
\fi

\iffalse
\begin{frame}{Why Convex Optimization?}
	\BIT
	\item Beautiful, fairly complete, and useful theory
	%\pause
	\item Solution algorithms that work well in theory and practice
	\BIT
	\item Convex optimization is \textbf{actionable}
	\EIT
	%\pause
	\item \textbf{Many applications} in
	\BIT
	\item Control
	\item Combinatorial optimization
	\item Signal and image processing
	\item Communications, networks
	\item Circuit design
	\item Machine learning, statistics
	\item Finance
	\EIT
	%\ldots and many more
	\EIT
\end{frame}
\fi

\iffalse
\begin{frame}{How do you Solve a Convex Problem?}
	\BIT\itemsep 20pt
	\item Use an existing custom solver for your specific problem
	%(\eg, SVM, lasso)
	%\item write your own (custom) solver
	%\BIT
	%\item lots of work, but can take advantage of special structure
	%\item commonly done in machine learning
	%\EIT
	\item Develop a new solver for your problem using a currently
	fashionable method
	%(mirror descent, Nesterov acceleration, Frank-Wolf \ldots)
	\BIT
	\item Requires work
	\item But (with luck) will scale to large problems
	\EIT
	\item Transform your problem into a cone program, and use a standard cone program solver
	\BIT
	\item Can be \emph{automated} using \emph{domain specific languages}
	\item CVX, YALMIP, CVXPY, Convex.jl
	\EIT
	
	%\BIT
	%\item extends reach of problems solvable by standard solvers
	%\item transformation can be hard to find, cumbersome to carry out
	%\EIT
	%\item \textbf{this talk:} methods to formalize and automate last
	%approach
	\EIT
\end{frame}
\fi

\section{CVXR}

\begin{frame}{CVXR}
	% Math isn't new. What's new is ability to implement in a few lines of a clear code.
	A modeling language in R for convex optimization
	
	\BIT
		\item Open source down to the solvers
		\item Uses disciplined convex programming to verify convexity
		\item Supports parameters, multiple constraints
		\item Mixes easily with general R code and other libraries
	\EIT
\end{frame}

\section{Examples}

\begin{frame}{Example: Ordinary Least Squares (OLS)}
	TODO: Mathematical definition of problem?
\end{frame}

\begin{frame}[fragile]{Example: Ordinary Least Squares (OLS)}
	\begin{verbatim}
	library(cvxr)
	beta <- Variable(n)
	obj <- SumSquares(y - X %*% beta)
	prob <- Problem(Minimize(obj))
	solution <- solve(prob)
	solution$opt_val
	solution$beta
	\end{verbatim}
	
	\BIT
		\item \verb|X| and \verb|y| are constants; \verb|beta|, \verb|obj|, and \verb|prob| are S4 objects
		\item \verb|solve| method canonicalizes, solves, and returns a list with final objective and optimal value of each variable
	\EIT
\end{frame}

\begin{frame}[fragile]{Example: Non-Negative Least Squares}
	\begin{verbatim}
	constr <- list(beta >= 0)
	prob2 <- Problem(Minimize(obj), constr)
	solution2 <- solve(prob2)
	solution2$opt_val
	solution2$beta
	\end{verbatim}
	
	\BIT
		\item Extend prior example by requiring \verb|beta| be non-negative
		\item Construct new problem with list \verb|constr| of constraints formed from constants and variables
		\item Variables, parameters, expressions, and constraints exist outside of any problem
	\EIT
\end{frame}

\begin{frame}{Results: Non-Negative Least Squares}
	\vfill
	\begin{figure}
		\includegraphics[width=1\textwidth]{figs/nnls.png}
	\end{figure}
	\vfill
\end{frame}

\begin{frame}[fragile]{Overview: Huber Regression}
	\[
	\begin{array}{ll}
	\mbox{minimize} & \sum_i^n \phi(y_i - \beta^T x_i)
	\end{array}
	\]
	with variable $\beta \in \reals^n$ and Huber function
	\[
	\begin{array}{ll}
	\phi(u) &= \begin{cases} u^2 & |u| \leq 0 \\
							2Mu - M^2 & |u| > 0 \end{cases}
	\end{array}
	\]
	where $M > 0$ is the threshold
	\BIT
		\item TODO: Plot of Huber function?
		\item Same as OLS for small residuals, allows some large residuals
		\item Better fit when data contains outliers
	\EIT
\end{frame}

\begin{frame}[fragile]{Example: Huber Regression}
	\begin{verbatim}
	beta = Variable(n)
	cost = SumEntries(Huber(y - X %*% beta, 1))
	prob <- Problem(Minimize(cost))
	solution <- solve(prob)
	solution$opt_val
	solution$beta
	\end{verbatim}
	
	\BIT
		\item Generate data and replace fraction $p$ of $y_i$'s with $-y_i$
		\item Huber regression with threshold of 1, no constraints
		\item Compare with OLS and prescient regression where sign changes are known
	\EIT
\end{frame}

\begin{frame}{Results: Huber Regression}
	\vfill
	\begin{figure}
		\includegraphics[width=1\textwidth]{figs/huber1.png}
	\end{figure}
	\vfill
\end{frame}

\begin{frame}{Results: Huber Regression}
	\vfill
	\begin{figure}
		\includegraphics[width=1\textwidth]{figs/huber2.png}
	\end{figure}
	\vfill
\end{frame}

\begin{frame}{Overview: Direct Standardization}
	\BIT
	\item TODO: Overview of problem
	\EIT
\end{frame}

\begin{frame}{Overview: Direct Standardization}
	\[
	\begin{array}{ll} \mbox{maximize} & \sum_i p_i \log p_i \\
	\mbox{subject to} & p \geq 0, \sum_i^n p_i = 1\\
	& Ap = b
	\end{array}
	\]
	with variable $p \in \reals^n$
	
	\BIT
	\item Probabilities $p$ must be non-negative and sum to 1
	\item $Ap = b$ represents distributional knowledge about some attributes, e.g. expected value, CDF in given range, etc
	\item Maximizing negative entropy without constraint $Ap = b$ yields uniform distribution
	\EIT
\end{frame}

\begin{frame}{Example: Direct Standardization}
	\BIT
	\item TODO: Specific data we generate for example
	\EIT
\end{frame}

\begin{frame}[fragile]{Example: Direct Standardization}
	\begin{verbatim}
	probs <- Variable(n)
	cost <- SumEntries(Entr(probs))
	constr <- list(probs >= 0, SumEntries(probs) == 1, 
				   t(X[,1]) %*% probs == 0.5)
	prob <- Problem(Maximize(cost), constr)
	solution <- solve(prob)
	solution$probs
	\end{verbatim}
	
	\BIT
		\item TODO: Details about code, constraint on fraction of women
	\EIT
\end{frame}

\begin{frame}{Results: Direct Standardization}
	TODO: Plot of CDF from original, skewed, and estimated distributions
\end{frame}

\section{Disciplined Convex Programming}

\iffalse
\begin{frame}{Curvature: Convex, Concave, and Affine functions}
	\includegraphics[width=0.9\textwidth,height=0.20\textheight]{figs/curvature.png}
	\BIT
		\item $f$ is \emph{concave} if $-f$ is convex, \ie, for any $x,y$,
		$\theta \in [0,1]$,
		\[
		f(\theta x + (1-\theta) y) \geq \theta f(x) + (1-\theta) f(y)
		\]
		\item $f$ is \emph{affine} if it is convex and concave, \ie,
		\[
		f(\theta x + (1-\theta) y) = \theta f(x) + (1-\theta) f(y)
		\]
		for any $x,y$, $\theta \in [0,1]$
		\item $f$ is affine $\Longleftrightarrow$ it has form
		$f(x) = a^Tx +b$
	\EIT
\end{frame}
\fi

\iffalse
\begin{frame}{Verifying a Function is Convex or Concave}
	(Verifying affine is easy)
	
	\vfill
	Approaches:
	\BIT
		\item Via basic definition (inequality)
		\item Via first or second order conditions, \eg, $\nabla^2 f(x) \succeq 0$
		
		\vfill
		
		\item Via convex calculus: construct $f$ using
		\BIT
			\item Library of basic functions that are convex or concave
			\item Calculus rules or transformations that preserve convexity
		\EIT
	\EIT
\end{frame}
\fi

\begin{frame}{Disciplined Convex Programming (DCP)}
	(\emph{Grant, Boyd, Ye, 2006})
	
	\vfill
	\BIT
		\item Framework for describing convex optimization problems
		\item Based on constructive convex analysis
		\item Sufficient, but not necessary for convexity
		\item Basis for several domain-specific languages and tools for convex optimization
		\BIT
			\item CVX, YALMIP, CVXPY, Convex.jl
		\EIT
	\EIT
	\vfill
\end{frame}

\section{Future Work}
\begin{frame}{Future Work}
	\BIT
		\item More solvers - SCS, POGS, etc
		\item More convex functions and constraints
		\item Domain-specific language for defining new operators
		\item Warm start to speed up convergence
	\EIT
\end{frame}

\end{document}